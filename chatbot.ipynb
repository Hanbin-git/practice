{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7EyoDWFW49erRGC7QLK+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/practice/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcDfAlOdppdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "from get_namuwiki_docs import load_namuwiki_docs_selenium\n",
        "from langchain.schema import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import streamlit as st\n",
        "\n",
        "# with open(\"key.json\", 'r') as file:\n",
        "#     data = json.load(file)\n",
        "\n",
        "# gemini_api_key = data.get(\"gemini-key\")\n",
        "\n",
        "# TODO: 아래 YOUR-HUGGINGFACE-API-KEY랑 OUR-GEMINI-API-KEY에 자기꺼 넣기\n",
        "if not os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-HUGGINGFACE-API-KEY\"\n",
        "gemini_api_key = \"YOUR-GEMINI-API-KEY\"\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "# gemini 모델 로드\n",
        "def load_model():\n",
        "    with st.spinner(\"모델을 로딩하는 중...\"):\n",
        "        gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"Model loaded...\")\n",
        "    return gemini_model\n",
        "\n",
        "# 임베딩 로드\n",
        "def load_embedding():\n",
        "    with st.spinner(\"임베딩을 로딩하는 중...\"):\n",
        "        embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "    print(\"Embedding loaded...\")\n",
        "    return embedding\n",
        "\n",
        "# Faiss vector DB 생성\n",
        "def create_vectorstore(topic):\n",
        "    with st.spinner(\"나무위키에서 문서를 가져오는 중...\"):\n",
        "        text = load_namuwiki_docs_selenium(topic)\n",
        "        # st.write(f\"찾은 문서 예시:\\n{text[:100]}\")\n",
        "\n",
        "    if text:\n",
        "        paragraphs = text.split(\"\\n\\n\")[:-1] if \"\\n\\n\" in text else text.split(\"\\n\")\n",
        "    else:\n",
        "        paragraphs = []\n",
        "\n",
        "    # FAISS 벡터 스토어 생성\n",
        "    with st.spinner(\"벡터 스토어를 생성하는 중...\"):\n",
        "        # convert to Document object (required for LangChain)\n",
        "        documents = [Document(page_content=doc, metadata={\"source\": f\"doc{idx+1}\"}) for idx, doc in enumerate(paragraphs)]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "\n",
        "        vectorstore = FAISS.from_documents(documents=splits, embedding=st.session_state.embedding)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# RAG using prompt\n",
        "def rag_chatbot(question):\n",
        "    context_docs = st.session_state.vectorstore.similarity_search(question, k=2)\n",
        "    # for i, doc in enumerate(context_docs):\n",
        "    #     st.write(f\"{i+1}번째 문서: {doc.page_content}\")\n",
        "\n",
        "    context_docs = \"\\n\\n\".join([f\"{i+1}번째 문서:\\n{doc.page_content}\" for i, doc in enumerate(context_docs)])\n",
        "\n",
        "    # prompt = f\"Context: {context_docs}\\nQuestion: {question}\\nAnswer in a complete sentence:\"\n",
        "    prompt = f\"문맥: {context_docs}\\n질문: {question}\\n답변:\"\n",
        "    # response = gemini_model(prompt)\n",
        "\n",
        "    response = st.session_state.model.generate_content(prompt)\n",
        "    answer = response.candidates[0].content.parts[0].text\n",
        "\n",
        "    print(\"출처 문서:\", context_docs)\n",
        "    return answer, context_docs\n",
        "\n",
        "\n",
        "# Streamlit 세션에서 모델을 한 번만 로드하도록 설정\n",
        "# 1. gemini model\n",
        "if \"model\" not in st.session_state:\n",
        "    st.session_state.model = load_model()\n",
        "\n",
        "# 2. embedding model\n",
        "if \"embedding\" not in st.session_state:\n",
        "    st.session_state.embedding = load_embedding()\n",
        "\n",
        "# 세션의 대화 히스토리 초기화\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "if \"topic\" not in st.session_state:\n",
        "    st.session_state.topic = \"\"\n",
        "\n",
        "\n",
        "# 1. 이 주제로 Vectorstore 만들 문서 가져오기\n",
        "topic = st.text_input('찾을 문서의 주제를 입력하세요. 예시) 흑백요리사: 요리 계급 전쟁(시즌 1)')\n",
        "\n",
        "if st.button('문서 가져오기'):\n",
        "    if topic:\n",
        "        vectorstore = create_vectorstore(topic)\n",
        "        st.session_state.vectorstore = vectorstore\n",
        "        st.session_state.topic = topic\n",
        "    else:\n",
        "        st.warning('주제를 입력해라', icon=\"⚠️\")\n",
        "\n",
        "if st.session_state.topic and st.session_state.vectorstore:\n",
        "    st.write(f\"주제: '{st.session_state.topic}' 로 Vectorstore 준비완료\")\n",
        "\n",
        "\n",
        "# 2. 사용자 질문에 유사한 내용을 Vectorstore에서 RAG 기반으로 답변\n",
        "user_query = st.text_input('질문을 입력하세요.')\n",
        "\n",
        "if st.button('질문하기') and user_query:\n",
        "    # 사용자의 질문을 히스토리에 추가\n",
        "    st.session_state.chat_history.append(f\"[user]: {user_query}\")\n",
        "    st.text(f'[You]: {user_query}')\n",
        "\n",
        "    # response = st.session_state.model.generate_content(user_querie)\n",
        "    # model_response = response.candidates[0].content.parts[0].text\n",
        "\n",
        "    # 모델 응답 RAG\n",
        "    if st.session_state.vectorstore:\n",
        "        response, context_docs = rag_chatbot(user_query)\n",
        "        st.text(f'[Chatbot]: {response}')\n",
        "        st.text(f'출처 문서:\\n')\n",
        "        st.write(context_docs)\n",
        "    else:\n",
        "        response = \"vector store is not ready.\"\n",
        "        st.text(f'[Chatbot]: {response}')\n",
        "\n",
        "    # 모델 응답을 히스토리에 추가\n",
        "    st.session_state.chat_history.append(f\"[chatbot]: {response}\")\n",
        "\n",
        "    # 전체 히스토리 출력\n",
        "    st.text(\"Chat History\")\n",
        "    st.text('--------------------------------------------')\n",
        "    st.text(\"\\n\".join(st.session_state.chat_history))\n"
      ]
    }
  ]
}