{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2a144bc133a4cadb0bd88df315829cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1aeff89e86146f3a602bff22293c32b",
              "IPY_MODEL_925e4e8ba5de4a85bdfc9e2573e8e3c8",
              "IPY_MODEL_f93adddffcb94861b40faa213ef6f2a5"
            ],
            "layout": "IPY_MODEL_eeab2f2ad218407ebdc8290ab94e292e"
          }
        },
        "c1aeff89e86146f3a602bff22293c32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_184d7e80ed444cc6bf0d724a361b6eb2",
            "placeholder": "​",
            "style": "IPY_MODEL_4035039db2554df59f00fe79657682fe",
            "value": "model.safetensors: 100%"
          }
        },
        "925e4e8ba5de4a85bdfc9e2573e8e3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d1a934832b4fe1bb721ffc43641c76",
            "max": 122330162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9df59c6104784af7811fad7c10883e5a",
            "value": 122330162
          }
        },
        "f93adddffcb94861b40faa213ef6f2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f02b628e681845039e56908debee7662",
            "placeholder": "​",
            "style": "IPY_MODEL_6a4e1e9875564bbe93a1af05e77602b0",
            "value": " 122M/122M [00:00&lt;00:00, 212MB/s]"
          }
        },
        "eeab2f2ad218407ebdc8290ab94e292e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184d7e80ed444cc6bf0d724a361b6eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4035039db2554df59f00fe79657682fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d1a934832b4fe1bb721ffc43641c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9df59c6104784af7811fad7c10883e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f02b628e681845039e56908debee7662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4e1e9875564bbe93a1af05e77602b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/practice/blob/main/feature_color%2BRandAugment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAK3Rh5GcJ9P",
        "outputId": "023d465e-a764-42c4-c020-82879ceeb2dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# 1. Drive에서 로컬로 복사 (빠름)\n",
        "shutil.copy(\"/content/drive/MyDrive/open.zip\", \"/content/open.zip\")\n",
        "\n",
        "# 2. 압축 풀기\n",
        "with zipfile.ZipFile(\"/content/open.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# 3. 확인\n",
        "!ls /content/train\n"
      ],
      "metadata": {
        "id": "VGLGwzmFf9U8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5067a72d-b122-4529-9654-5450427f30d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "컨티넨탈_10세대_2017_2019\t     A_클래스_W177_2020_2025\n",
            "어코드_10세대_2018_2022\t\t     B_클래스_W246_2013_2018\n",
            "1시리즈_F20_2013_2015\t\t     뉴_스타일_코란도_C_2017_2019\n",
            "1시리즈_F20_2016_2019\t\t     프리우스_C_2018_2020\n",
            "1시리즈_F40_2020_2024\t\t     뉴_CC_2012_2016\n",
            "그랜드카니발_2006_2010\t\t     CLA_클래스_C117_2014_2019\n",
            "2008_2015_2017\t\t\t     CLA_클래스_C118_2020_2025\n",
            "에쿠스_신형_2010_2015\t\t     CLE_클래스_C236_2024_2025\n",
            "파나메라_2010_2016\t\t     CLS_클래스_C257_2019_2023\n",
            "뉴_제타_2011_2016\t\t     CLS_클래스_W218_2012_2017\n",
            "뉴_카이엔_2011_2018\t\t     아반떼_하이브리드_CN7_2021_2023\n",
            "엑센트_신형_2011_2019\t\t     아반떼_CN7_2021_2023\n",
            "스파크_2012_2015\t\t     더_뉴_아반떼_CN7_2023_2025\n",
            "쿠퍼_컨트리맨_2012_2015\t\t     CT6_2016_2018\n",
            "올_뉴_모닝_2012_2015\t\t     C_클래스_W204_2008_2015\n",
            "말리부_2012_2016\t\t     C_클래스_W205_2015_2021\n",
            "아베오_2012_2016\t\t     C_클래스_W206_2022_2024\n",
            "뉴_티구안_2012_2016\t\t     제네시스_DH_2014_2016\n",
            "레이_2012_2017\t\t\t     쏘나타_DN8_2020_2023\n",
            "올란도_2012_2018\t\t     쏘나타_디_엣지_DN8_2024_2025\n",
            "더_뉴_파사트_2012_2019\t\t     e_트론_2020_2023\n",
            "트랙스_2013_2016\t\t     E_PACE_2018_2020\n",
            "콰트로포르테_2014_2016\t\t     EQ900_2016_2018\n",
            "더_뉴_아반떼_2014_2016\t\t     EQA_H243_2021_2024\n",
            "마칸_2014_2018\t\t\t     EQE_V295_2022_2024\n",
            "그랜드_체로키_2014_2020\t\t     EQS_V297_2022_2023\n",
            "기블리_2014_2023\t\t     뉴_ES300h_2013_2015\n",
            "더_뉴_모닝_2015_2016\t\t     뉴_ES300h_2016_2018\n",
            "레니게이드_2015_2017\t\t     ES300h_7세대_2019_2026\n",
            "올_뉴_쏘렌토_2015_2017\t\t     디_올뉴니로EV_2023_2024\n",
            "티볼리_2015_2018\t\t     더_기아_레이_EV_2024_2025\n",
            "아슬란_2015_2018\t\t     EV6_2022_2024\n",
            "디스커버리_스포츠_2015_2019\t     EV9_2024_2025\n",
            "올_뉴_카니발_2015_2019\t\t     E_클래스_W212_2010_2016\n",
            "에스컬레이드_2015_2020\t\t     E_클래스_W213_2017_2020\n",
            "머스탱_2015_2023\t\t     E_클래스_W213_2021_2023\n",
            "익스플로러_2016_2017\t\t     E_클래스_W214_2024_2025\n",
            "그랜드_스타렉스_2016_2018\t     F150_2004_2021\n",
            "싼타페_더_프라임_2016_2018\t     F_PACE_2017_2019\n",
            "더_넥스트_스파크_2016_2018\t     G4_렉스턴_2018_2020\n",
            "더_뉴_맥스크루즈_2016_2018\t     G70_2018_2020\n",
            "더_뉴_코란도_스포츠_2016_2018\t     더_뉴_G70_2021_2025\n",
            "레인지로버_이보크_2016_2019\t     G80_2017_2020\n",
            "아이오닉_하이브리드_2016_2019\t     더_올뉴G80_2021_2024\n",
            "티볼리_에어_2016_2019\t\t     뉴_G80_2025_2026\n",
            "임팔라_2016_2019\t\t     G80_RG3_2021_2023\n",
            "쿠퍼_컨버터블_2016_2024\t\t     G80_RG3_2025\n",
            "쿠퍼_컨트리맨_2016_2024\t\t     G90_2019_2022\n",
            "쿠퍼_클럽맨_2016_2024\t\t     G90_RS4_2022_2025\n",
            "알티마_2017_2018\t\t     GLA_클래스_H247_2020_2025\n",
            "올_뉴_말리부_2017_2018\t\t     GLA_클래스_X156_2015_2019\n",
            "올_뉴_카마로_2017_2018\t\t     GLB_클래스_X247_2020_2023\n",
            "니로_2017_2019\t\t\t     GLC_클래스_X253_2017_2019\n",
            "더_뉴_모하비_2017_2019\t\t     GLC_클래스_X253_2020_2022\n",
            "콰트로포르테_2017_2022\t\t     GLC_클래스_X253_2023\n",
            "르반떼_2017_2022\t\t     GLC_클래스_X254_2023_2025\n",
            "더_뉴_트랙스_2017_2022\t\t     GLE_클래스_W166_2016_2018\n",
            "레인지로버_벨라_2018_2019\t     GLE_클래스_W167_2019_2024\n",
            "익스플로러_2018_2019\t\t     GLS_클래스_X166_2017_2019\n",
            "티볼리_아머_2018_2019\t\t     GLS_클래스_X167_2020_2024\n",
            "쏘나타_뉴_라이즈_2018_2019\t     그랜저_GN7_2023_2025\n",
            "스팅어_2018_2020\t\t     컨티넨탈_GT_2세대_2012_2017\n",
            "스토닉_2018_2020\t\t     컨티넨탈_GT_3세대_2018_2023\n",
            "코나_2018_2020\t\t\t     파사트_GT_B8_2018_2022\n",
            "더_뉴_쏘렌토_2018_2020\t\t     GV70_2021_2023\n",
            "렉스턴_스포츠_2018_2021\t\t     일렉트리파이드_GV70_2022_2024\n",
            "더_뉴_그랜드_스타렉스_2018_2021      GV80_2020_2022\n",
            "더_뉴_레이_2018_2022\t\t     뉴_GV80_2024_2025\n",
            "티구안_올스페이스_2018_2023\t     GV80_2024_2025\n",
            "아테온_2018_2023\t\t     G_클래스_W463_2009_2017\n",
            "넥쏘_2018_2024\t\t\t     G_클래스_W463b_2019_2025\n",
            "렉스턴_스포츠_칸_2019_2020\t     그랜저_HG_2011_2014\n",
            "더_뉴_카니발_2019_2020\t\t     그랜저_HG_2015_2017\n",
            "마칸_2019_2021\t\t\t     i30_PD_2017_2018\n",
            "팰리세이드_2019_2022\t\t     i4_2022_2024\n",
            "스포티지_더_볼드_2019_2022\t     그랜저_IG_2017_2019\n",
            "더_뉴_스파크_2019_2022\t\t     더_뉴_그랜저_IG_2020_2023\n",
            "더_뉴_말리부_2019_2022\t\t     iX_2022_2024\n",
            "레니게이드_2019_2023\t\t     올_뉴_모닝_JA_2017_2020\n",
            "뷰티풀_코란도_2019_2024\t\t     모닝_어반_JA_2021_2023\n",
            "더_뉴_아이오닉_하이브리드_2020\t     더_뉴_모닝_JA_2024_2025\n",
            "콜로라도_2020_2020\t\t     랭글러_JK_2009_2017\n",
            "코세어_2020_2022\t\t     랭글러_JL_2018_2024\n",
            "더_뉴_니로_2020_2022\t\t     벨로스터_JS_2018_2020\n",
            "트래버스_2020_2023\t\t     글래디에이터_JT_2020_2023\n",
            "셀토스_2020_2023\t\t     K3_2013_2015\n",
            "베리_뉴_티볼리_2020_2023\t     더_뉴_K3_2016_2018\n",
            "모하비_더_마스터_2020_2024\t     올_뉴_K3_2019_2021\n",
            "베뉴_2020_2024\t\t\t     더_뉴_K3_2세대_2022_2024\n",
            "트레일블레이저_2021_2022\t     K5_2세대_2016_2018\n",
            "티볼리_에어_2021_2022\t\t     더_뉴_K5_2세대_2019_2020\n",
            "리얼_뉴_콜로라도_2021_2022\t     K5_3세대_하이브리드_2020_2022\n",
            "스팅어_마이스터_2021_2023\t     K5_하이브리드_3세대_2020_2023\n",
            "더_올뉴투싼_하이브리드_2021_2023     K5_3세대_2020_2023\n",
            "더_뉴_싼타페_2021_2023\t\t     더_뉴_K5_하이브리드_3세대_2023_2025\n",
            "더_뉴_코나_2021_2023\t\t     더_뉴_K5_3세대_2024_2025\n",
            "타이칸_2021_2025\t\t     더_뉴_K7_2013_2016\n",
            "더_뉴_렉스턴_스포츠_칸_2021_2025     올_뉴_K7_2016_2019\n",
            "더_뉴_렉스턴_스포츠_2021_2025\t     올_뉴_K7_하이브리드_2017_2019\n",
            "올_뉴_렉스턴_2021_2025\t\t     K7_프리미어_하이브리드_2020_2021\n",
            "캐스퍼_2022_2024\t\t     K7_프리미어_2020_2021\n",
            "마칸_2022_2024\t\t\t     K8_하이브리드_2022_2024\n",
            "디_올_뉴_스포티지_2022_2024\t     K8_2022_2024\n",
            "스타리아_2022_2025\t\t     더_K9_2019_2021\n",
            "디_올뉴니로_2022_2025\t\t     더_뉴_K9_2세대_2022_2025\n",
            "더_뉴_기아_레이_2022_2025\t     체로키_KL_2019_2023\n",
            "디_올_뉴_니로_2022_2025\t\t     디펜더_L663_2020_2025\n",
            "트레일블레이저_2023\t\t     LF_쏘나타_2015_2017\n",
            "더_뉴_팰리세이드_2023_2024\t     팰리세이드_LX3_2025\n",
            "토레스_2023_2025\t\t     M2_F87_2016_2021\n",
            "디_올뉴그랜저_2023_2025\t\t     M4_F82_2015_2020\n",
            "디_올뉴코나_2023_2025\t\t     M5_F90_2018_2023\n",
            "더_뉴_셀토스_2023_2025\t\t     아반떼_MD_2011_2014\n",
            "트랙스_크로스오버_2024_2025\t     MKC_2015_2018\n",
            "디_올뉴싼타페_2024_2025\t\t     뉴_MKZ_2017_2020\n",
            "그랑_콜레오스_2025\t\t     싼타페_MX5_2024_2025\n",
            "레인지로버_스포츠_2세대_2013_2017    아반떼_N_2022_2023\n",
            "레인지로버_스포츠_2세대_2018_2022    New_XF_2012_2015\n",
            "컴패스_2세대_2018_2022\t\t     투싼_NX4_2021_2023\n",
            "레인지로버_이보크_2세대_2020_2022    더_뉴_투싼_NX4_2023_2025\n",
            "디스커버리_스포츠_2세대_2020_2025    카이엔_PO536_2019_2023\n",
            "에비에이터_2세대_2020_2025\t     Q30_2017_2019\n",
            "레인지로버_이보크_2세대_2023_2024    Q3_F3_2020_2024\n",
            "액티언_2세대_2025\t\t     Q50_2014_2017\n",
            "2시리즈_그란쿠페_F44_2020_2024\t     Q5_FY_2020\n",
            "2시리즈_액티브_투어러_F45_2019_2021  Q5_FY_2021_2024\n",
            "2시리즈_액티브_투어러_U06_2022_2024  Q7_4M_2016_2019\n",
            "3008_2세대_2018_2023\t\t     Q7_4M_2020_2023\n",
            "파일럿_3세대_2016_2018\t\t     Q8_4M_2020_2025\n",
            "모델_3_2019_2022\t\t     QM3_2014_2017\n",
            "투아렉_3세대_2020_2023\t\t     뉴QM3_2018_2019\n",
            "모델_3_2024_2025\t\t     뉴_QM5_2012_2014\n",
            "3시리즈_E90_2005_2012\t\t     QM6_2017_2019\n",
            "3시리즈_F30_2013_2018\t\t     더_뉴_QM6_2020_2023\n",
            "3시리즈_G20_2019_2022\t\t     뉴_QM6_2021_2023\n",
            "3시리즈_G20_2023_2025\t\t     더_뉴_QM6_2024_2025\n",
            "3시리즈_GT_F34_2014_2021\t     QX60_2016_2018\n",
            "디스커버리_4_2010_2016\t\t     뉴쏘렌토_R_2013_2014\n",
            "레인지로버_4세대_2014_2017\t     더_뉴스포티지R_2014_2016\n",
            "몬데오_4세대_2015_2020\t\t     RAV4_2016_2018\n",
            "프리우스_4세대_2016_2018\t     RAV4_5세대_2019_2024\n",
            "스포티지_4세대_2016_2018\t     S60_3세대_2020_2024\n",
            "레인지로버_4세대_2018_2022\t     S90_2017_2020\n",
            "프리우스_4세대_2019_2022\t     S90_2021_2025\n",
            "카니발_4세대_2021\t\t     SM3_네오_2015_2019\n",
            "쏘렌토_4세대_2021_2023\t\t     뉴_SM5_임프레션_2008_2010\n",
            "시에나_4세대_2021_2024\t\t     뉴_SM5_플래티넘_2013_2014\n",
            "카니발_4세대_2022_2023\t\t     SM5_노바_2015_2019\n",
            "더_뉴_쏘렌토_4세대_2024_2025\t     SM6_2016_2020\n",
            "더_뉴_카니발_4세대_2024_2025\t     더_뉴_SM6_2021_2024\n",
            "라브4_4세대_2013_2018\t\t     SM7_뉴아트_2008_2011\n",
            "라브4_5세대_2019_2024\t\t     SM7_노바_2015_2019\n",
            "4시리즈_F32_2014_2020\t\t     S_클래스_W221_2006_2013\n",
            "4시리즈_G22_2021_2023\t\t     S_클래스_W222_2014_2020\n",
            "4시리즈_G22_2024_2025\t\t     S_클래스_W223_2021_2025\n",
            "5008_2세대_2018_2019\t\t     코나_SX2_2023_2025\n",
            "5008_2세대_2021_2024\t\t     그랜저TG_2007_2008\n",
            "디스커버리_5_2017_2020\t\t     올_뉴_투싼_TL_2016_2018\n",
            "에스컬레이드_5세대_2021_2024\t     올_뉴_투싼_TL_2019_2020\n",
            "아이오닉5_2022_2023\t\t     싼타페_TM_2019_2020\n",
            "디스커버리_5_2022_2024\t\t     UX250h_2019_2024\n",
            "스포티지_5세대_2022_2024\t     V40_2015_2018\n",
            "레인지로버_5세대_2023_2024\t     V60_크로스컨트리_2세대_2020_2025\n",
            "5시리즈_F10_2010_2016\t\t     V90_크로스컨트리_2018_2024\n",
            "5시리즈_G30_2017_2023\t\t     뉴_체어맨_W_2012_2016\n",
            "5시리즈_G60_2024_2025\t\t     그랜드_체로키_WL_2021_2023\n",
            "5시리즈_GT_F07_2010_2017\t     X1_F48_2016_2019\n",
            "익스플로러_6세대_2020_2025\t     X1_F48_2020_2022\n",
            "아이오닉6_2023_2025\t\t     X1_U11_2023_2024\n",
            "6시리즈_F12_2011_2018\t\t     X2_F39_2018_2023\n",
            "6시리즈_GT_G32_2018_2020\t     X3_G01_2018_2021\n",
            "6시리즈_GT_G32_2021_2024\t     X3_G01_2022_2024\n",
            "박스터_718_2017_2024\t\t     X4_F26_2015_2018\n",
            "718_카이맨_2017_2024\t\t     X4_G02_2019_2021\n",
            "718_박스터_2017_2024\t\t     X4_G02_2022_2025\n",
            "골프_7세대_2013_2016\t\t     X5_F15_2014_2018\n",
            "7시리즈_F01_2009_2015\t\t     X5_G05_2019_2023\n",
            "7시리즈_G11_2016_2018\t\t     X5_G05_2024_2025\n",
            "7시리즈_G11_2019_2022\t\t     X6_F16_2015_2019\n",
            "7시리즈_G70_2023_2025\t\t     X6_G06_2020_2023\n",
            "8시리즈_G15_2020_2024\t\t     X6_G06_2024_2025\n",
            "911_2003_2019\t\t\t     X7_G07_2019_2022\n",
            "911_992_2020_2024\t\t     X7_G07_2023_2025\n",
            "파나메라_971_2017_2023\t\t     XC40_2019_2022\n",
            "A4_B9_2016_2019\t\t\t     XC60_2세대_2018_2021\n",
            "A4_B9_2020_2024\t\t\t     XC60_2세대_2022_2025\n",
            "A5_F5_2019_2024\t\t\t     XC90_2세대_2017_2019\n",
            "뉴_A6_2012_2014\t\t\t     XC90_2세대_2020_2025\n",
            "뉴_A6_2015_2018\t\t\t     XE_2016_2019\n",
            "A6_C8_2019_2025\t\t\t     XF_X260_2016_2020\n",
            "A7_2012_2016\t\t\t     XJ_8세대_2010_2019\n",
            "A7_4K_2020_2024\t\t\t     XM3_2020_2023\n",
            "A8_D5_2018_2023\t\t\t     XM3_2024\n",
            "아반떼_AD_2016_2018\t\t     캠리_XV70_2018_2024\n",
            "더_뉴_아반떼_AD_2019_2020\t     모델_Y_2021_2025\n",
            "All_New_XJ_2016_2019\t\t     YF쏘나타_2009_2012\n",
            "AMG_GT_2016_2024\t\t     YF쏘나타_하이브리드_2011_2015\n",
            "A_클래스_W176_2015_2018\t\t     Z4_G29_2019_2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "metadata": {
        "id": "eyLBp4j3i6F1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 경로\n",
        "train_dir = '/content/train'\n",
        "\n",
        "# test 경로\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# sample_submission (추론 때 사용)\n",
        "sample_submission_path = '/content/sample_submission.csv'\n"
      ],
      "metadata": {
        "id": "Ybtf0x5si9CE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def show_memory_status():\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB 단위\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)    # MB 단위\n",
        "    print(f\"📊 현재 GPU 메모리 상태: Allocated = {allocated:.2f} MB | Reserved = {reserved:.2f} MB\")\n",
        "\n",
        "# 현재 CUDA 사용 가능 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    print(\"🔍 초기화 전 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "\n",
        "    # GPU 캐시 및 메모리 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n🧹 GPU 메모리 초기화 완료\")\n",
        "    print(\"🔍 초기화 후 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "else:\n",
        "    print(\"❌ CUDA 사용 불가\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxuQLgBAmkIm",
        "outputId": "203c95ad-7bcf-4547-8a89-fd4f92d3db46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 초기화 전 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 0.00 MB | Reserved = 0.00 MB\n",
            "\n",
            "🧹 GPU 메모리 초기화 완료\n",
            "🔍 초기화 후 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 0.00 MB | Reserved = 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # 🚗 Step 1: Load Image (RGB 고정)\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # 🚗 Step 2: Calculate Features if needed\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = np.array([width / height], dtype=np.float32)\n",
        "\n",
        "        image_cv2 = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "        color_mean = image_cv2.mean(axis=(0, 1))\n",
        "        color_mean = color_mean[::-1]\n",
        "        color_mean = np.array(color_mean / 255.0, dtype=np.float32)\n",
        "\n",
        "        # 🚗 Step 3: Apply Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)  # fallback\n",
        "\n",
        "        # 🚗 Step 4: Extract label\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 🚗 Step 5: Return according to mode\n",
        "        if self.use_aspect and self.use_color:\n",
        "            return image, torch.tensor(aspect_ratio), torch.tensor(color_mean), label\n",
        "        elif self.use_aspect:\n",
        "            return image, torch.tensor(aspect_ratio), label\n",
        "        elif self.use_color:\n",
        "            return image, torch.tensor(color_mean), label\n",
        "        else:\n",
        "            return image, label\n"
      ],
      "metadata": {
        "id": "rtI0lYOuJZt5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import RandAugment\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ✅ 실험 이름\n",
        "EXPERIMENT = \"Baseline_RandAug\"\n",
        "\n",
        "# ✅ 파일 리스트 로딩\n",
        "file_list = glob.glob('/content/train/*/*.jpg')\n",
        "\n",
        "# ✅ 클래스명 추출 함수\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "# ✅ 클래스 인덱스 매핑\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"✅ 클래스 수: {len(class_to_idx)}\")  # 396 expected\n",
        "\n",
        "# ✅ 라벨 리스트\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "# ✅ Train/Val Split\n",
        "train_files, val_files = train_test_split(file_list, test_size=0.1, stratify=labels, random_state=42)\n",
        "\n",
        "# ✅ Step2 기준 Transform (기본 + RandAug 포함)\n",
        "IMG_SIZE = 456\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    RandAugment(num_ops=2, magnitude=9),  # ✅ RandAug 추가\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# ✅ Dataset 클래스\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # Load image\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Feature: aspect ratio\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = torch.tensor([width / height], dtype=torch.float32)\n",
        "\n",
        "        # Feature: RGB 평균값 (color mean)\n",
        "        image_np = np.array(image_pil)\n",
        "        color_mean = image_np.mean(axis=(0, 1)) / 255.0\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        # Transform\n",
        "        assert self.transform is not None, \"transform은 반드시 지정되어야 합니다\"\n",
        "        image = self.transform(image_pil)\n",
        "\n",
        "        # Label\n",
        "        class_name = extract_class_name_jpg(path)\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 반환 조건에 따라 조정\n",
        "        if self.use_aspect and self.use_color:\n",
        "            return image, aspect_ratio, color_mean, label\n",
        "        elif self.use_aspect:\n",
        "            return image, aspect_ratio, label\n",
        "        elif self.use_color:\n",
        "            return image, color_mean, label\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "# ✅ 실험 설정 (Step2: RandAugment)\n",
        "USE_ASPECT = False\n",
        "USE_COLOR = True\n",
        "\n",
        "print(f\"🚀 실험 설정: {EXPERIMENT} (Aspect={USE_ASPECT}, Color={USE_COLOR})\")\n",
        "\n",
        "# ✅ Dataset 및 DataLoader\n",
        "train_dataset = CarImageDataset(train_files, class_to_idx, train_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "val_dataset   = CarImageDataset(val_files, class_to_idx, val_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n"
      ],
      "metadata": {
        "id": "u2DB9tCSJcmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a87668-d155-4410-b2ab-9e02d29c4963"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 클래스 수: 396\n",
            "🚀 실험 설정: Baseline_RandAug (Aspect=False, Color=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "# ✅ 실험명\n",
        "EXPERIMENT = \"Baseline_RandAug\"\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect, use_color, num_classes):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "        # ✅ EfficientNet-B5 (no drop_connect here)\n",
        "        self.backbone = timm.create_model(\n",
        "            'tf_efficientnet_b5',\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "        backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        # ✅ 메타 feature 차원 계산\n",
        "        meta_features_dim = 0\n",
        "        if self.use_aspect:\n",
        "            meta_features_dim += 1\n",
        "        if self.use_color:\n",
        "            meta_features_dim += 3\n",
        "\n",
        "        # ✅ Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(backbone_out_features + meta_features_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),  # Step1의 Dropout 유지\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None):\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        aux_list = []\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ 실험 설정 (RandAug 실험)\n",
        "USE_ASPECT = False\n",
        "USE_COLOR = True\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# ✅ 모델 생성\n",
        "model = CustomModel(USE_ASPECT, USE_COLOR, NUM_CLASSES).to(device)\n",
        "\n",
        "# ✅ Step2 전략: backbone은 freeze, FC만 학습\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "tNalQaGrKS5u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "c2a144bc133a4cadb0bd88df315829cf",
            "c1aeff89e86146f3a602bff22293c32b",
            "925e4e8ba5de4a85bdfc9e2573e8e3c8",
            "f93adddffcb94861b40faa213ef6f2a5",
            "eeab2f2ad218407ebdc8290ab94e292e",
            "184d7e80ed444cc6bf0d724a361b6eb2",
            "4035039db2554df59f00fe79657682fe",
            "a1d1a934832b4fe1bb721ffc43641c76",
            "9df59c6104784af7811fad7c10883e5a",
            "f02b628e681845039e56908debee7662",
            "6a4e1e9875564bbe93a1af05e77602b0"
          ]
        },
        "outputId": "828867d6-6f32-4e7d-e8bb-53c2eaab638a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/122M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a144bc133a4cadb0bd88df315829cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AdamW + weight_decay 추가 추천 (EffNet 계열에 많이 사용)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "JNIgNO9AKcl1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet-B5 with Color Feature + Dropout\n",
        "# Strategy: Baseline + Dropout + EarlyStopping + KFold (5)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "# ✅ Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ File paths and labels\n",
        "file_list = glob.glob('/content/train/*/*.jpg')\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "# ✅ Transforms\n",
        "IMG_SIZE = 456\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Feature extractors\n",
        "def compute_aspect_ratio(path):\n",
        "    with Image.open(path) as img:\n",
        "        w, h = img.size\n",
        "        return w / h\n",
        "\n",
        "def compute_dominant_color(path):\n",
        "    with Image.open(path).convert(\"RGB\") as img:\n",
        "        img = img.resize((16, 16))\n",
        "        np_img = np.array(img) / 255.0\n",
        "        return np_img.mean(axis=(0, 1))\n",
        "\n",
        "# ✅ Dataset\n",
        "class CarJPGDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.class_to_idx[extract_class_name_jpg(path)]\n",
        "\n",
        "        meta_features = []\n",
        "        if self.use_aspect:\n",
        "            meta_features.append(compute_aspect_ratio(path))\n",
        "        if self.use_color:\n",
        "            meta_features.extend(compute_dominant_color(path).tolist())\n",
        "\n",
        "        meta_features = torch.tensor(meta_features if meta_features else [0.0, 0.0, 0.0], dtype=torch.float32)\n",
        "        return image, meta_features, label\n",
        "\n",
        "# ✅ Model\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect, use_color, num_classes):\n",
        "        super().__init__()\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "        self.backbone = timm.create_model('tf_efficientnet_b5', pretrained=True, num_classes=0)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        meta_dim = (1 if use_aspect else 0) + (3 if use_color else 0)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.backbone.num_features + meta_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None):\n",
        "        x = self.backbone(image)\n",
        "        aux = []\n",
        "        if self.use_aspect:\n",
        "            aux.append(aspect_ratio)\n",
        "        if self.use_color:\n",
        "            aux.append(color_mean)\n",
        "        if aux:\n",
        "            x = torch.cat([x] + aux, dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ✅ Configs\n",
        "EXPERIMENT = \"C\"\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "use_aspect = False\n",
        "use_color = True\n",
        "\n",
        "# ✅ Training loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n📂 Fold {fold + 1}/5\")\n",
        "\n",
        "    train_files = [file_list[i] for i in train_idx]\n",
        "    val_files = [file_list[i] for i in val_idx]\n",
        "\n",
        "    train_dataset = CarJPGDataset(train_files, class_to_idx, train_transform, use_aspect, use_color)\n",
        "    val_dataset = CarJPGDataset(val_files, class_to_idx, val_transform, use_aspect, use_color)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=96, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=96, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = CustomModel(use_aspect, use_color, num_classes=396).to(device)\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(1, 31):\n",
        "        print(f\"\\n🔄 Epoch {epoch}\")\n",
        "\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0.0, 0\n",
        "        for X, meta, y in tqdm(train_loader, desc=\"Train\", leave=False):\n",
        "            X, meta, y = X.to(device), meta.to(device), y.to(device)\n",
        "            outputs = model(X, color_mean=meta)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, meta, y in tqdm(val_loader, desc=\"Valid\", leave=False):\n",
        "                X, meta, y = X.to(device), meta.to(device), y.to(device)\n",
        "                outputs = model(X, color_mean=meta)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_path = f\"/content/drive/MyDrive/team_models/EffNetB5_{EXPERIMENT}_fold{fold+1}.pth\"\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"📦 Model saved for Fold {fold+1}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⚠️ EarlyStopping patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"⛔ Early stopping\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"🎯 Best model for Fold {fold+1} loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZJkMSSKeiG",
        "outputId": "e52daeac-0f09-4918-a907-e2382d42720f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📂 Fold 1/5\n",
            "\n",
            "🔄 Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  23%|██▎       | 65/277 [02:33<08:16,  2.34s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ 고정 경로\n",
        "TEST_DIR = \"/content/test\"\n",
        "SAMPLE_SUB_PATH = \"/content/sample_submission.csv\"\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# ✅ 샘플 제출 파일에서 클래스명 추출\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' 제외\n",
        "\n",
        "# ✅ Transform (Step2 기준: 456x456 + RandAugment)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((456, 456)),\n",
        "    transforms.RandAugment(),  # Step2 핵심 추가\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Dominant Color 계산 함수\n",
        "def compute_dominant_color(path):\n",
        "    with Image.open(path).convert(\"RGB\") as img:\n",
        "        img = img.resize((16, 16))\n",
        "        np_img = np.array(img) / 255.0\n",
        "        mean_color = np_img.mean(axis=(0, 1))\n",
        "        return mean_color  # (3,)\n",
        "\n",
        "# ✅ 테스트용 Dataset\n",
        "class TestJPGDatasetWithColor(Dataset):\n",
        "    def __init__(self, img_root, transform=None):\n",
        "        self.file_list = sorted([\n",
        "            os.path.join(img_root, f) for f in os.listdir(img_root) if f.endswith('.jpg')\n",
        "        ])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        color_mean = compute_dominant_color(path)\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        return image, color_mean, fname\n",
        "\n",
        "# ✅ DataLoader 설정\n",
        "test_dataset = TestJPGDatasetWithColor(TEST_DIR, transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=6,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=4\n",
        ")\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ 실험명 및 모델 정의 import\n",
        "exp_name = \"step2\"  # 변경됨\n",
        "from model import CustomModel  # 반드시 학습에 사용한 CustomModel과 동일하게 정의되어 있어야 함\n",
        "\n",
        "FOLD_MODEL_PATHS = [\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold{fold}.pth\"\n",
        "    for fold in range(1, 6)\n",
        "]\n",
        "\n",
        "ensemble_outputs = []\n",
        "\n",
        "for fold_idx, model_path in enumerate(FOLD_MODEL_PATHS):\n",
        "    print(f\"\\n🚀 Inference with Fold {fold_idx + 1} Model: {model_path}\")\n",
        "\n",
        "    model = CustomModel(use_aspect=False, use_color=True, num_classes=NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    fold_probs = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, color_mean, names in tqdm(test_loader, desc=f\"🔍 Fold {fold_idx + 1} Inference\"):\n",
        "            imgs = imgs.to(device)\n",
        "            color_mean = color_mean.to(device)\n",
        "            outputs = model(imgs, color_mean=color_mean)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            fold_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    fold_probs = np.concatenate(fold_probs, axis=0)\n",
        "    ensemble_outputs.append(fold_probs)\n",
        "\n",
        "# ✅ 앙상블 평균 결과 저장\n",
        "ensemble_outputs = np.mean(np.stack(ensemble_outputs, axis=0), axis=0)\n",
        "\n",
        "results = []\n",
        "for idx, path in enumerate(test_dataset.file_list):\n",
        "    fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "    row = {\"ID\": fname}\n",
        "    row.update({class_name: ensemble_outputs[idx, i] for i, class_name in enumerate(column_names)})\n",
        "    results.append(row)\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df = submission_df[[\"ID\"] + column_names]\n",
        "\n",
        "SAVE_SUBMISSION_PATH = f\"/content/drive/MyDrive/team_models/submission_effb5_step2.csv\"\n",
        "submission_df.to_csv(SAVE_SUBMISSION_PATH, index=False)\n",
        "print(f\"\\n✅ 앙상블 서브미션 저장 완료: {SAVE_SUBMISSION_PATH}\")\n"
      ],
      "metadata": {
        "id": "k61x77gkNSDG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uNfZUpDQiNxe"
      }
    }
  ]
}