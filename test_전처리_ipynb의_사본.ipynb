{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1dd684qSyNKG-SEUtvV1zuUQSfCsW0ANS",
      "authorship_tag": "ABX9TyNBhbEZYnljHqTu80eLWHAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "956db5ac7bbb4eebb589418a309ad837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1cb40e358b74de78fb1a64bee56b847",
              "IPY_MODEL_13a31fb744e24a68ba0aa6f7396e074d",
              "IPY_MODEL_9493eb0ff3dc4ab3a36395ce7cc4a242"
            ],
            "layout": "IPY_MODEL_c729cc26c26847f39492ea8737e0fab9"
          }
        },
        "b1cb40e358b74de78fb1a64bee56b847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_735254c638604abf888612285b70dbf2",
            "placeholder": "​",
            "style": "IPY_MODEL_1dbb39f80ae04635a001e905cafb1142",
            "value": "model.safetensors: 100%"
          }
        },
        "13a31fb744e24a68ba0aa6f7396e074d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2296a15d13654d139317c5305e58fd53",
            "max": 114374272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15e25a5062ee4f6a8ee1147e196a7ace",
            "value": 114374272
          }
        },
        "9493eb0ff3dc4ab3a36395ce7cc4a242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42fad4ac3cdd432382122baa0981ca2b",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb91e01236c43b9b9e87718ef244f96",
            "value": " 114M/114M [00:01&lt;00:00, 96.2MB/s]"
          }
        },
        "c729cc26c26847f39492ea8737e0fab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735254c638604abf888612285b70dbf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbb39f80ae04635a001e905cafb1142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2296a15d13654d139317c5305e58fd53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15e25a5062ee4f6a8ee1147e196a7ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42fad4ac3cdd432382122baa0981ca2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb91e01236c43b9b9e87718ef244f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/practice/blob/main/test_%EC%A0%84%EC%B2%98%EB%A6%AC_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "peaMNLWchXQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juS4rlAhexwK",
        "outputId": "f0a10edc-09e8-4c70-d3f8-b73a5eb3ed2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF"
      ],
      "metadata": {
        "id": "eyLBp4j3i6F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 설정 =====\n",
        "test_csv_path = '/content/drive/MyDrive/open/test.csv'\n",
        "test_img_dir = '/content/drive/MyDrive/open/test'  # test 이미지 폴더\n",
        "save_root = '/content/drive/MyDrive/fuck/preprocessed_test'  # test 전처리 저장 위치\n",
        "os.makedirs(save_root, exist_ok=True)"
      ],
      "metadata": {
        "id": "Ybtf0x5si9CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Grayscale + Resize transform 정의\n",
        "# class SquarePad:\n",
        "#     def __call__(self, image):\n",
        "#         w, h = image.size\n",
        "#         max_wh = max(w, h)\n",
        "#         pad_left = (max_wh - w) // 2\n",
        "#         pad_top = (max_wh - h) // 2\n",
        "#         padding = (pad_left, pad_top, max_wh - w - pad_left, max_wh - h - pad_top)\n",
        "#         return TF.pad(image, padding, fill=0)\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Grayscale(num_output_channels=1),\n",
        "#     SquarePad(),\n",
        "#     transforms.Resize((384, 384)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.5], [0.5])\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "Dxb4JBcejAnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # ===== Test CSV 읽기 =====\n",
        "# test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# # ✅ 수정된 부분: img_path에서 파일명만 추출\n",
        "# test_df['img_path'] = test_df['img_path'].apply(lambda x: os.path.basename(x))\n",
        "# test_df['full_img_path'] = test_df['img_path'].apply(lambda x: os.path.join(test_img_dir, x))\n",
        "# test_df['img_name_no_ext'] = test_df['img_path'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])  # 'TEST_00000'\n",
        "\n",
        "# # ===== 전처리 시작 =====\n",
        "# counter = 0  # 전체 이미지 카운터\n",
        "# batch_index = 0  # batch_000, batch_001...\n",
        "# batch_limit = 800  # 한 폴더에 저장할 최대 개수\n",
        "\n",
        "# current_batch_dir = os.path.join(save_root, f\"batch_{batch_index:03}\")\n",
        "# os.makedirs(current_batch_dir, exist_ok=True)\n",
        "\n",
        "# print(\"📦 Test 전처리 시작...\")\n",
        "\n",
        "# for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "#     img_id = row['img_name_no_ext']  # TEST_00000\n",
        "#     img_path = row['full_img_path']\n",
        "\n",
        "#     try:\n",
        "#         img = Image.open(img_path).convert('RGB')\n",
        "#         img_tensor = transform(img)\n",
        "\n",
        "#         # 폴더 분할 조건\n",
        "#         if counter > 0 and counter % batch_limit == 0:\n",
        "#             batch_index += 1\n",
        "#             current_batch_dir = os.path.join(save_root, f\"batch_{batch_index:03}\")\n",
        "#             os.makedirs(current_batch_dir, exist_ok=True)\n",
        "\n",
        "#         # 저장 파일명: TEST_00000.npy\n",
        "#         save_name = f\"{img_id}.npy\"\n",
        "#         save_path = os.path.join(current_batch_dir, save_name)\n",
        "#         np.save(save_path, img_tensor.numpy())\n",
        "\n",
        "#         counter += 1\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ 오류: {img_path} → {e}\")\n",
        "\n",
        "# print(f\"✅ Test 전처리 완료: 총 {counter}개 이미지 저장됨\")\n",
        "# print(f\"📂 배치 폴더 수: {batch_index + 1}\")"
      ],
      "metadata": {
        "id": "SqM_vaNwjLHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # 🚗 Test용 npy 파일 경로 (예시)\n",
        "# npy_path = '/content/drive/MyDrive/fuck/preprocessed_test/batch_000/TEST_00000.npy'\n",
        "\n",
        "# # npy 불러오기\n",
        "# img = np.load(npy_path)\n",
        "\n",
        "# # (1, H, W) → (H, W)\n",
        "# if img.shape[0] == 1:\n",
        "#     img = img.squeeze(0)\n",
        "\n",
        "# # 시각화\n",
        "# plt.imshow(img, cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.title(\"Test Grayscale 시각화\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "spSREKsVZ9R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Test JPG 파일 수 확인\n",
        "# jpg_count = 0\n",
        "# jpg_paths = []\n",
        "\n",
        "# test_dir = '/content/drive/MyDrive/open/test'\n",
        "# for file in os.listdir(test_dir):\n",
        "#     if file.lower().endswith('.jpg'):\n",
        "#         jpg_count += 1\n",
        "#         jpg_paths.append(file)  # 파일명만 저장 (TEST_00000.jpg 등)\n",
        "\n",
        "# print(f\"✅ Test 폴더 내 JPG 파일 수: {jpg_count}\")\n",
        "\n",
        "# # 전처리된 .npy 파일 수 확인\n",
        "# npy_dir = '/content/drive/MyDrive/fuck/preprocessed_test'\n",
        "# npy_count = 0\n",
        "# npy_files = []\n",
        "\n",
        "# # 모든 하위 폴더까지 탐색\n",
        "# for root, dirs, files in os.walk(npy_dir):\n",
        "#     for file in files:\n",
        "#         if file.endswith('.npy'):\n",
        "#             npy_count += 1\n",
        "#             npy_files.append(os.path.join(root, file))\n",
        "\n",
        "# print(f\"✅ 저장된 .npy 파일 수: {npy_count}\")\n",
        "\n",
        "# # ⛔ 중복 또는 추가 파일 의심 확인\n",
        "# # jpg_paths = ['TEST_00000.jpg', ...] → 대응되는 npy는 'TEST_00000.npy'\n",
        "# expected_npy_names = set([os.path.splitext(f)[0] + '.npy' for f in jpg_paths])\n",
        "# actual_npy_names = set([os.path.basename(f) for f in npy_files])\n",
        "\n",
        "# extra = actual_npy_names - expected_npy_names\n",
        "# missing = expected_npy_names - actual_npy_names\n",
        "\n",
        "# if len(extra) == 0 and len(missing) == 0:\n",
        "#     print(\"✅ 모든 파일 정상! JPG ↔ npy 개수 및 이름 일치 🎉\")\n",
        "# else:\n",
        "#     print(f\"⚠️ 의심되는 추가 저장 파일 수: {len(extra)}\")\n",
        "#     print(\"예시 파일 (extra):\", list(extra)[:5])\n",
        "#     print(f\"⚠️ 누락된 예상 파일 수: {len(missing)}\")\n",
        "#     print(\"예시 파일 (missing):\", list(missing)[:5])\n"
      ],
      "metadata": {
        "id": "hcIid1rQ4fCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "model = timm.create_model('convnext_tiny', pretrained=True, num_classes=396)\n",
        "print(model.default_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "956db5ac7bbb4eebb589418a309ad837",
            "b1cb40e358b74de78fb1a64bee56b847",
            "13a31fb744e24a68ba0aa6f7396e074d",
            "9493eb0ff3dc4ab3a36395ce7cc4a242",
            "c729cc26c26847f39492ea8737e0fab9",
            "735254c638604abf888612285b70dbf2",
            "1dbb39f80ae04635a001e905cafb1142",
            "2296a15d13654d139317c5305e58fd53",
            "15e25a5062ee4f6a8ee1147e196a7ace",
            "42fad4ac3cdd432382122baa0981ca2b",
            "eeb91e01236c43b9b9e87718ef244f96"
          ]
        },
        "id": "95uHM81tFoKn",
        "outputId": "f232a510-65e9-41ba-8a7a-692a99b02cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "956db5ac7bbb4eebb589418a309ad837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'url': '', 'hf_hub_id': 'timm/convnext_tiny.in12k_ft_in1k', 'architecture': 'convnext_tiny', 'tag': 'in12k_ft_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'test_input_size': (3, 288, 288), 'fixed_input_size': False, 'interpolation': 'bicubic', 'crop_pct': 0.95, 'test_crop_pct': 1.0, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 1000, 'pool_size': (7, 7), 'first_conv': 'stem.0', 'classifier': 'head.fc'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model.default_cfg)\n"
      ],
      "metadata": {
        "id": "IZ9btXdawzHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def show_memory_status():\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB 단위\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)    # MB 단위\n",
        "    print(f\"📊 현재 GPU 메모리 상태: Allocated = {allocated:.2f} MB | Reserved = {reserved:.2f} MB\")\n",
        "\n",
        "# 현재 CUDA 사용 가능 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    print(\"🔍 초기화 전 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "\n",
        "    # GPU 캐시 및 메모리 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n🧹 GPU 메모리 초기화 완료\")\n",
        "    print(\"🔍 초기화 후 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "else:\n",
        "    print(\"❌ CUDA 사용 불가\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bH_WHzciHHx",
        "outputId": "2bee1269-2e84-47e4-ae46-b711325cf570"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 초기화 전 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 644.61 MB | Reserved = 29950.00 MB\n",
            "\n",
            "🧹 GPU 메모리 초기화 완료\n",
            "🔍 초기화 후 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 644.61 MB | Reserved = 1086.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 설정\n",
        "IMAGE_SIZE = 384\n",
        "ORIGIN_DIR = '/content/drive/MyDrive/open/test'\n",
        "SAVE_DIR = '/content/drive/MyDrive/open/clean_test'\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "image_paths = glob(f'{ORIGIN_DIR}/*.jpg')\n",
        "\n",
        "def resize_with_padding(img, size=384):\n",
        "    h, w = img.shape\n",
        "    scale = size / max(h, w)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "    resized = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "    # 상하좌우 여백 계산\n",
        "    delta_w = size - new_w\n",
        "    delta_h = size - new_h\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "\n",
        "    # 패딩: 검은색(0)\n",
        "    padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
        "    return padded\n",
        "\n",
        "for img_path in tqdm(image_paths, desc=\"📦 전처리 중 (padding 방식)\"):\n",
        "    try:\n",
        "        # Grayscale 로드\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            print(f\"❌ 이미지 로드 실패: {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # 비율 유지 + 패딩\n",
        "        img = resize_with_padding(img, size=IMAGE_SIZE)\n",
        "\n",
        "        # 정규화 (-1 ~ 1)\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = (img - 0.5) / 0.5\n",
        "        img = np.expand_dims(img, axis=0)  # (1, 384, 384)\n",
        "\n",
        "        # 저장 경로\n",
        "        class_name = os.path.basename(os.path.dirname(img_path))\n",
        "        file_name = os.path.basename(img_path).replace('.jpg', '.npy')\n",
        "        save_class_dir = os.path.join(SAVE_DIR, class_name)\n",
        "        os.makedirs(save_class_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(save_class_dir, file_name), img)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류 발생: {img_path} - {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv2kiYL9-RSb",
        "outputId": "eb3b7374-27dc-45f5-bd1c-b3ba4214aa4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "📦 전처리 중 (padding 방식): 100%|██████████| 8258/8258 [05:27<00:00, 25.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def extract_class_name(fname):\n",
        "    # '1시리즈_F20_2013_2015_000123.npy' → '1시리즈_F20_2013_2015'\n",
        "    return '_'.join(os.path.basename(fname).replace('.npy', '').split('_')[:-1])\n",
        "\n",
        "# ✅ 현재 train 경로\n",
        "all_npy_files = glob.glob('/content/drive/MyDrive/open/clean_dataset/**/*.npy', recursive=True)\n",
        "\n",
        "class_names = sorted(set(extract_class_name(f) for f in all_npy_files))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"클래스 수: {len(class_to_idx)}\")  # 396개 나와야 정상\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OMK20ohGQU1",
        "outputId": "6597aeac-f0e4-4bba-a4f4-c7f944336dd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스 수: 387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dataset 클래스 정의\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CarNPYDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # 🚀 Step 1. Load npy → tensor\n",
        "        image = np.load(path, mmap_mode='r')\n",
        "        image = torch.from_numpy(image).float()  # (1, H, W)\n",
        "\n",
        "        # 🚀 Step 2. Grayscale(1) → RGB(3)\n",
        "        image = image.repeat(3, 1, 1)  # (3, H, W)\n",
        "\n",
        "        # 🚀 Step 3. Label 추출 (!!! 여기가 반드시 있어야 함)\n",
        "        class_name = '_'.join(os.path.basename(path).replace('.npy', '').split('_')[:-1])\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 🚀 Step 4. Transform 적용\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 🚀 Step 5. Return (이 시점에 label이 반드시 정의되어야 함)\n",
        "        return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "rtI0lYOuJZt5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 .npy 파일 리스트 + train/val split + DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 전체 .npy 파일\n",
        "file_list = glob.glob('/content/drive/MyDrive/open/clean_dataset/**/*.npy', recursive=True)\n",
        "\n",
        "# 라벨 추출 → stratify 기반 split\n",
        "labels = [class_to_idx[extract_class_name(f)] for f in file_list]\n",
        "train_files, val_files = train_test_split(file_list, test_size=0.1, stratify=labels, random_state=42)\n",
        "\n",
        "# transform\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((384, 384)),  # ✅ 384로 변경\n",
        "    # transforms.ToTensor(),  # 이미 Tensor라면 빼기\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "train_dataset = CarNPYDataset(train_files, class_to_idx, transform)\n",
        "val_dataset = CarNPYDataset(val_files, class_to_idx, transform)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n"
      ],
      "metadata": {
        "id": "u2DB9tCSJcmI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=396)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "tNalQaGrKS5u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AdamW + weight_decay 추가 추천 (EffNet 계열에 많이 사용)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "JNIgNO9AKcl1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import copy\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ StratifiedKFold 정의\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ✅ 5-Fold 루프 시작\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🔁 Fold {fold + 1} / 5\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # ✅ Fold별 train/val split\n",
        "    train_files = [file_list[i] for i in train_idx]\n",
        "    val_files = [file_list[i] for i in val_idx]\n",
        "\n",
        "    # ✅ Fold별 Dataset & DataLoader\n",
        "    train_dataset = CarNPYDataset(train_files, class_to_idx, transform)\n",
        "    val_dataset = CarNPYDataset(val_files, class_to_idx, transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    # ✅ Fold별 model / criterion / optimizer 초기화\n",
        "    model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=396)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    # ✅ EarlyStopping 변수 초기화\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # ✅ Epoch 루프\n",
        "    for epoch in range(1, 31):\n",
        "        print(f\"\\n📌 Fold {fold+1} | Epoch {epoch}\")\n",
        "\n",
        "        # === 학습 ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Train Fold {fold+1}\", leave=False)\n",
        "        for X, y in loop:\n",
        "            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # === 검증 ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        val_loop = tqdm(val_loader, desc=f\"Valid Fold {fold+1}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loop:\n",
        "                X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "                val_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # === 로그 출력 ===\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # === EarlyStopping ===\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_path = f\"/content/drive/MyDrive/team_models/EffNetB5_fold{fold+1}.pth\"\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"📦 Best model saved for Fold {fold+1}!\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⚠️ EarlyStopping patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"⛔ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # ✅ Fold 끝나고 Best 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"✅ Fold {fold+1} Best model loaded.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZJkMSSKeiG",
        "outputId": "6cd9783a-75f8-422d-d31f-d197ccaea473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "🔁 Fold 1 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "📌 Fold 1 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Fold 1:   0%|          | 0/809 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "Valid Fold 1:   0%|          | 0/203 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 1 | Train Loss: 2.0013 | Acc: 0.5985\n",
            "✅ Fold 1 | Epoch 1 | Val   Loss: 0.4340 | Acc: 0.8768\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 2 | Train Loss: 0.2645 | Acc: 0.9192\n",
            "✅ Fold 1 | Epoch 2 | Val   Loss: 0.3092 | Acc: 0.9096\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 3 | Train Loss: 0.1401 | Acc: 0.9533\n",
            "✅ Fold 1 | Epoch 3 | Val   Loss: 0.3221 | Acc: 0.9097\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 1 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 4 | Train Loss: 0.1325 | Acc: 0.9596\n",
            "✅ Fold 1 | Epoch 4 | Val   Loss: 0.3078 | Acc: 0.9150\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 5 | Train Loss: 0.1001 | Acc: 0.9693\n",
            "✅ Fold 1 | Epoch 5 | Val   Loss: 0.2594 | Acc: 0.9323\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 6 | Train Loss: 0.0974 | Acc: 0.9718\n",
            "✅ Fold 1 | Epoch 6 | Val   Loss: 0.2799 | Acc: 0.9281\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 1 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 7 | Train Loss: 0.0808 | Acc: 0.9761\n",
            "✅ Fold 1 | Epoch 7 | Val   Loss: 0.2783 | Acc: 0.9256\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 1 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 8 | Train Loss: 0.0728 | Acc: 0.9796\n",
            "✅ Fold 1 | Epoch 8 | Val   Loss: 0.2460 | Acc: 0.9388\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 9 | Train Loss: 0.0729 | Acc: 0.9793\n",
            "✅ Fold 1 | Epoch 9 | Val   Loss: 0.2448 | Acc: 0.9392\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 10 | Train Loss: 0.0645 | Acc: 0.9831\n",
            "✅ Fold 1 | Epoch 10 | Val   Loss: 0.2601 | Acc: 0.9338\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 1 | Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 11 | Train Loss: 0.0610 | Acc: 0.9833\n",
            "✅ Fold 1 | Epoch 11 | Val   Loss: 0.2441 | Acc: 0.9403\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 12 | Train Loss: 0.0470 | Acc: 0.9875\n",
            "✅ Fold 1 | Epoch 12 | Val   Loss: 0.2227 | Acc: 0.9457\n",
            "📦 Best model saved for Fold 1!\n",
            "\n",
            "📌 Fold 1 | Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 13 | Train Loss: 0.0514 | Acc: 0.9876\n",
            "✅ Fold 1 | Epoch 13 | Val   Loss: 0.3010 | Acc: 0.9290\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 1 | Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 14 | Train Loss: 0.0548 | Acc: 0.9860\n",
            "✅ Fold 1 | Epoch 14 | Val   Loss: 0.2712 | Acc: 0.9385\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 1 | Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 1 | Epoch 15 | Train Loss: 0.0517 | Acc: 0.9865\n",
            "✅ Fold 1 | Epoch 15 | Val   Loss: 0.2282 | Acc: 0.9437\n",
            "⚠️ EarlyStopping patience: 3/3\n",
            "⛔ Early stopping triggered.\n",
            "✅ Fold 1 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "🔁 Fold 2 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "📌 Fold 2 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Fold 2:   0%|          | 0/809 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "Valid Fold 2:   0%|          | 0/203 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 1 | Train Loss: 2.0334 | Acc: 0.5911\n",
            "✅ Fold 2 | Epoch 1 | Val   Loss: 0.4315 | Acc: 0.8745\n",
            "📦 Best model saved for Fold 2!\n",
            "\n",
            "📌 Fold 2 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 2 | Train Loss: 0.2606 | Acc: 0.9218\n",
            "✅ Fold 2 | Epoch 2 | Val   Loss: 0.2762 | Acc: 0.9154\n",
            "📦 Best model saved for Fold 2!\n",
            "\n",
            "📌 Fold 2 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 3 | Train Loss: 0.1471 | Acc: 0.9529\n",
            "✅ Fold 2 | Epoch 3 | Val   Loss: 0.3099 | Acc: 0.9125\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 2 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 4 | Train Loss: 0.1193 | Acc: 0.9637\n",
            "✅ Fold 2 | Epoch 4 | Val   Loss: 0.2956 | Acc: 0.9111\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 2 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 5 | Train Loss: 0.1104 | Acc: 0.9659\n",
            "✅ Fold 2 | Epoch 5 | Val   Loss: 0.2739 | Acc: 0.9232\n",
            "📦 Best model saved for Fold 2!\n",
            "\n",
            "📌 Fold 2 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 6 | Train Loss: 0.0925 | Acc: 0.9726\n",
            "✅ Fold 2 | Epoch 6 | Val   Loss: 0.2163 | Acc: 0.9399\n",
            "📦 Best model saved for Fold 2!\n",
            "\n",
            "📌 Fold 2 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 7 | Train Loss: 0.0813 | Acc: 0.9758\n",
            "✅ Fold 2 | Epoch 7 | Val   Loss: 0.2866 | Acc: 0.9218\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 2 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 8 | Train Loss: 0.0768 | Acc: 0.9787\n",
            "✅ Fold 2 | Epoch 8 | Val   Loss: 0.2470 | Acc: 0.9318\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 2 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 2 | Epoch 9 | Train Loss: 0.0647 | Acc: 0.9816\n",
            "✅ Fold 2 | Epoch 9 | Val   Loss: 0.2316 | Acc: 0.9425\n",
            "⚠️ EarlyStopping patience: 3/3\n",
            "⛔ Early stopping triggered.\n",
            "✅ Fold 2 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "🔁 Fold 3 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "📌 Fold 3 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Fold 3:   0%|          | 0/809 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "Valid Fold 3:   0%|          | 0/203 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 1 | Train Loss: 1.9575 | Acc: 0.6058\n",
            "✅ Fold 3 | Epoch 1 | Val   Loss: 0.4284 | Acc: 0.8689\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 2 | Train Loss: 0.2537 | Acc: 0.9241\n",
            "✅ Fold 3 | Epoch 2 | Val   Loss: 0.3257 | Acc: 0.9043\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 3 | Train Loss: 0.1451 | Acc: 0.9553\n",
            "✅ Fold 3 | Epoch 3 | Val   Loss: 0.2763 | Acc: 0.9193\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 4 | Train Loss: 0.1143 | Acc: 0.9660\n",
            "✅ Fold 3 | Epoch 4 | Val   Loss: 0.3367 | Acc: 0.9075\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 3 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 5 | Train Loss: 0.1143 | Acc: 0.9665\n",
            "✅ Fold 3 | Epoch 5 | Val   Loss: 0.2779 | Acc: 0.9255\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 3 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 6 | Train Loss: 0.0850 | Acc: 0.9756\n",
            "✅ Fold 3 | Epoch 6 | Val   Loss: 0.2721 | Acc: 0.9303\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 7 | Train Loss: 0.0939 | Acc: 0.9736\n",
            "✅ Fold 3 | Epoch 7 | Val   Loss: 0.2468 | Acc: 0.9382\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 8 | Train Loss: 0.0710 | Acc: 0.9797\n",
            "✅ Fold 3 | Epoch 8 | Val   Loss: 0.3163 | Acc: 0.9188\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 3 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 9 | Train Loss: 0.0658 | Acc: 0.9829\n",
            "✅ Fold 3 | Epoch 9 | Val   Loss: 0.2582 | Acc: 0.9352\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 3 | Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 10 | Train Loss: 0.0667 | Acc: 0.9828\n",
            "✅ Fold 3 | Epoch 10 | Val   Loss: 0.2291 | Acc: 0.9425\n",
            "📦 Best model saved for Fold 3!\n",
            "\n",
            "📌 Fold 3 | Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 11 | Train Loss: 0.0458 | Acc: 0.9882\n",
            "✅ Fold 3 | Epoch 11 | Val   Loss: 0.2452 | Acc: 0.9460\n",
            "⚠️ EarlyStopping patience: 1/3\n",
            "\n",
            "📌 Fold 3 | Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 12 | Train Loss: 0.0531 | Acc: 0.9874\n",
            "✅ Fold 3 | Epoch 12 | Val   Loss: 0.2779 | Acc: 0.9298\n",
            "⚠️ EarlyStopping patience: 2/3\n",
            "\n",
            "📌 Fold 3 | Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 3 | Epoch 13 | Train Loss: 0.0591 | Acc: 0.9854\n",
            "✅ Fold 3 | Epoch 13 | Val   Loss: 0.2589 | Acc: 0.9388\n",
            "⚠️ EarlyStopping patience: 3/3\n",
            "⛔ Early stopping triggered.\n",
            "✅ Fold 3 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "🔁 Fold 4 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "📌 Fold 4 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Fold 4:   0%|          | 0/809 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "Valid Fold 4:   0%|          | 0/203 [00:00<?, ?it/s]<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n",
            "<ipython-input-10-10f0040b71bd>:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  image = torch.from_numpy(image).float()  # (1, H, W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fold 4 | Epoch 1 | Train Loss: 1.9658 | Acc: 0.6044\n",
            "✅ Fold 4 | Epoch 1 | Val   Loss: 0.4363 | Acc: 0.8698\n",
            "📦 Best model saved for Fold 4!\n",
            "\n",
            "📌 Fold 4 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid Fold 4:  11%|█▏        | 23/203 [00:04<00:25,  6.97it/s, loss=0.0215]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 앙상블 Inference 예시 (5개 모델 평균)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "# 설정\n",
        "FOLD_MODEL_PATHS = [\n",
        "    \"/content/drive/MyDrive/team_models/EffNetB5_fold1.pth\",\n",
        "    \"/content/drive/MyDrive/team_models/EffNetB5_fold2.pth\",\n",
        "    \"/content/drive/MyDrive/team_models/EffNetB5_fold3.pth\",\n",
        "    \"/content/drive/MyDrive/team_models/EffNetB5_fold4.pth\",\n",
        "    \"/content/drive/MyDrive/team_models/EffNetB5_fold5.pth\",\n",
        "]\n",
        "\n",
        "TEST_DIR = \"/content/drive/MyDrive/open/clean_test/test\"\n",
        "SAMPLE_SUB_PATH = \"/content/drive/MyDrive/open/sample_submission.csv\"\n",
        "SAVE_SUBMISSION_PATH = \"/content/drive/MyDrive/team_models/submission_fold5_ensemble.csv\"\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# 샘플 제출 파일에서 클래스명 추출\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' 제외\n",
        "\n",
        "# ✅ Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ 테스트용 Dataset\n",
        "class TestNPYDataset(Dataset):\n",
        "    def __init__(self, npy_root, transform=None):\n",
        "        self.file_list = []\n",
        "        for root, dirs, files in os.walk(npy_root):\n",
        "            for file in files:\n",
        "                if file.endswith('.npy'):\n",
        "                    self.file_list.append(os.path.join(root, file))\n",
        "        self.file_list.sort()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = np.load(path, mmap_mode='r').copy()\n",
        "        image = torch.from_numpy(image).float()\n",
        "        image = image.repeat(3, 1, 1)\n",
        "        image = image.contiguous()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".npy\", \"\")\n",
        "        return image, fname\n",
        "\n",
        "# ✅ DataLoader\n",
        "test_dataset = TestNPYDataset(TEST_DIR, transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=6,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=4\n",
        ")\n",
        "\n",
        "# ✅ 앙상블 결과 초기화\n",
        "ensemble_outputs = []\n",
        "\n",
        "# ✅ 각 Fold 모델 순서대로 추론\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for fold_idx, model_path in enumerate(FOLD_MODEL_PATHS):\n",
        "    print(f\"\\n🚀 Inference with Fold {fold_idx + 1} Model: {model_path}\")\n",
        "\n",
        "    # 모델 로드\n",
        "    model = timm.create_model('efficientnet_b5', pretrained=False, num_classes=NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # fold별 output 저장\n",
        "    fold_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, names in tqdm(test_loader, desc=f\"🔍 Fold {fold_idx + 1} Inference\"):\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            fold_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    fold_probs = np.concatenate(fold_probs, axis=0)\n",
        "    ensemble_outputs.append(fold_probs)\n",
        "\n",
        "# ✅ 앙상블 평균\n",
        "ensemble_outputs = np.stack(ensemble_outputs, axis=0)  # (num_folds, num_samples, num_classes)\n",
        "mean_outputs = np.mean(ensemble_outputs, axis=0)       # (num_samples, num_classes)\n",
        "\n",
        "# ✅ 결과 저장\n",
        "results = []\n",
        "for idx, path in enumerate(test_dataset.file_list):\n",
        "    fname = os.path.basename(path).replace(\".npy\", \"\")\n",
        "    row = {\"ID\": fname}\n",
        "    row.update({class_name: mean_outputs[idx, i] for i, class_name in enumerate(column_names)})\n",
        "    results.append(row)\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df = submission_df[[\"ID\"] + column_names]\n",
        "submission_df.to_csv(SAVE_SUBMISSION_PATH, index=False)\n",
        "\n",
        "print(f\"\\n✅ 앙상블 서브미션 저장 완료: {SAVE_SUBMISSION_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k61x77gkNSDG",
        "outputId": "434af969-6415-471b-d498-83e7926f2037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Inference: 100%|██████████| 130/130 [00:34<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 서브미션 저장 완료: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/submission.csv\")\n",
        "df.to_csv(\"submission_utf8sig.csv\", index=False, encoding='utf-8-sig')\n"
      ],
      "metadata": {
        "id": "ShGkYhOXRiw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}